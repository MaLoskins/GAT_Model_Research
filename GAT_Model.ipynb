{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning:\n",
      "\n",
      "`clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19872\\2669318039.py:42: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19872\\2669318039.py:44: UserWarning:\n",
      "\n",
      "Converting to PeriodArray/Index representation will drop timezone information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>geo</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>label</th>\n",
       "      <th>reply_to_tweet_id</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544267656597995521</td>\n",
       "      <td>258627226</td>\n",
       "      <td>KristyMayr7</td>\n",
       "      <td>1672</td>\n",
       "      <td>731</td>\n",
       "      <td>BREAKING: Hostages are being forced to hold an...</td>\n",
       "      <td>2014-12-14 23:08:15+00:00</td>\n",
       "      <td>445</td>\n",
       "      <td>54</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7NEWS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014Q4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544269152198721536</td>\n",
       "      <td>443573208</td>\n",
       "      <td>ottomanscribe</td>\n",
       "      <td>1192</td>\n",
       "      <td>607</td>\n",
       "      <td>@KristyMayr7 that is not an IS flag specifically.</td>\n",
       "      <td>2014-12-14 23:14:12+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristyMayr7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.442677e+17</td>\n",
       "      <td>2014Q4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>544269478406529024</td>\n",
       "      <td>2700238212</td>\n",
       "      <td>tlcrosemiller11</td>\n",
       "      <td>42</td>\n",
       "      <td>234</td>\n",
       "      <td>@KristyMayr7 omg I'm watching it on @morningsh...</td>\n",
       "      <td>2014-12-14 23:15:30+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristyMayr7,morningshowon7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.442677e+17</td>\n",
       "      <td>2014Q4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>544270016770633728</td>\n",
       "      <td>760246262</td>\n",
       "      <td>tihrigby</td>\n",
       "      <td>147</td>\n",
       "      <td>846</td>\n",
       "      <td>@KristyMayr7 @DeepPolitics dudes, not the flag...</td>\n",
       "      <td>2014-12-14 23:17:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristyMayr7,DeepPolitics</td>\n",
       "      <td>http://www.pri.org/sites/default/files/ISISfla...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.442677e+17</td>\n",
       "      <td>2014Q4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>544270351619067904</td>\n",
       "      <td>2670053628</td>\n",
       "      <td>SloaneSW7</td>\n",
       "      <td>299</td>\n",
       "      <td>614</td>\n",
       "      <td>@KristyMayr7 @bluebuzzbird How pathetic! These...</td>\n",
       "      <td>2014-12-14 23:18:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristyMayr7,bluebuzzbird</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.442677e+17</td>\n",
       "      <td>2014Q4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id     user_id user_screen_name  user_followers_count  \\\n",
       "0  544267656597995521   258627226      KristyMayr7                  1672   \n",
       "1  544269152198721536   443573208    ottomanscribe                  1192   \n",
       "2  544269478406529024  2700238212  tlcrosemiller11                    42   \n",
       "3  544270016770633728   760246262         tihrigby                   147   \n",
       "4  544270351619067904  2670053628        SloaneSW7                   299   \n",
       "\n",
       "   user_friends_count                                               text  \\\n",
       "0                 731  BREAKING: Hostages are being forced to hold an...   \n",
       "1                 607  @KristyMayr7 that is not an IS flag specifically.   \n",
       "2                 234  @KristyMayr7 omg I'm watching it on @morningsh...   \n",
       "3                 846  @KristyMayr7 @DeepPolitics dudes, not the flag...   \n",
       "4                 614  @KristyMayr7 @bluebuzzbird How pathetic! These...   \n",
       "\n",
       "                 created_at  retweet_count  favorite_count lang  geo hashtags  \\\n",
       "0 2014-12-14 23:08:15+00:00            445              54   en  NaN    7NEWS   \n",
       "1 2014-12-14 23:14:12+00:00              1               1   en  NaN      NaN   \n",
       "2 2014-12-14 23:15:30+00:00              0               0   en  NaN      NaN   \n",
       "3 2014-12-14 23:17:38+00:00              0               1   en  NaN      NaN   \n",
       "4 2014-12-14 23:18:58+00:00              0               0   en  NaN      NaN   \n",
       "\n",
       "                     mentions  \\\n",
       "0                         NaN   \n",
       "1                 KristyMayr7   \n",
       "2  KristyMayr7,morningshowon7   \n",
       "3    KristyMayr7,DeepPolitics   \n",
       "4    KristyMayr7,bluebuzzbird   \n",
       "\n",
       "                                                urls  label  \\\n",
       "0                                                NaN      1   \n",
       "1                                                NaN      1   \n",
       "2                                                NaN      1   \n",
       "3  http://www.pri.org/sites/default/files/ISISfla...      1   \n",
       "4                                                NaN      1   \n",
       "\n",
       "   reply_to_tweet_id Quarter  \n",
       "0                NaN  2014Q4  \n",
       "1       5.442677e+17  2014Q4  \n",
       "2       5.442677e+17  2014Q4  \n",
       "3       5.442677e+17  2014Q4  \n",
       "4       5.442677e+17  2014Q4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, HeteroConv\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# PARAMETERS\n",
    "#-------------------------\n",
    "DATA = r'CSV_Files\\sydneysiege.csv'\n",
    "TEXT = 'text'\n",
    "PLOT_COLOUR = 'label'\n",
    "NUM_POINTS = 3000\n",
    "EPOCHS = 200\n",
    "INCLUDE_STOPWORDS = True\n",
    "REMOVE_ATS = False\n",
    "EMBEDDING_DIM = 100\n",
    "LR = 0.0001\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EMBEDDING_METHOD = \"Glove\" # Options: \"Glove\", \"BERT\", \"BERT_Sentence\"\n",
    "#-------------------------\n",
    "\n",
    "# Load Venv in terminal: source .venv/Scripts/activate\n",
    "Data_Part = pd.read_csv(DATA)\n",
    "df = Data_Part.copy()\n",
    "\n",
    "\n",
    "# Convert 'Created_At' column to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')  # Convert to datetime, handle any conversion errors\n",
    "# Create a new column for quarterly periods\n",
    "df['Quarter'] = df['created_at'].dt.to_period('Q')  # Converts to quarterly periods (e.g., 2015Q1, 2015Q2)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, target_column=TEXT, contraction_dict=None):\n",
    "        self.target_column = target_column\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        \n",
    "        # Initialize spaCy model\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        default_stop_words = self.nlp.Defaults.stop_words\n",
    "        \n",
    "        important_words = {'not', 'no', 'never', 'none', 'nobody', 'nothing', 'nowhere', 'neither', 'nor', 'cannot', 'without', 'hardly', 'barely'}\n",
    "        self.stop_words = default_stop_words - important_words\n",
    "\n",
    "        # Dictionary has been taken for UWA NLP lab 5\n",
    "        self.contraction_dict = contraction_dict or  {\n",
    "            \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "            \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "            \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "            \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\",\n",
    "            \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
    "            \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "            \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "            \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "            \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "            \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "            \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "            \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
    "            \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "            \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "            \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "            \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "            \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
    "            \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "            \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "            \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
    "            \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "            \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "            \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "            \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "            \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "            \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "            \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "            \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "            \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "            \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "            \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "            \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "            \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "            \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "        }\n",
    "        \n",
    "        self.re_pattern = re.compile(r'[^\\w\\s]')  # Compile the regex once.\n",
    "        self.at_pattern = re.compile(r'@\\S+')\n",
    "\n",
    "\n",
    "    def clean_text(self, df):\n",
    "        # Regex Pattern and Lowercase\n",
    "        df[self.target_column] = df[self.target_column].str.lower()\n",
    "        if not REMOVE_ATS:\n",
    "            df[self.target_column] = df[self.target_column].str.replace(self.at_pattern, '', regex=True)\n",
    "\n",
    "        if self.contraction_dict:\n",
    "            for word, new_word in self.contraction_dict.items():\n",
    "                df[self.target_column] = df[self.target_column].str.replace(word, new_word, regex=False)\n",
    "        \n",
    "        # Regex for Punctuation Removal\n",
    "        df[self.target_column] = df[self.target_column].str.replace(self.re_pattern, '', regex=True)\n",
    "        \n",
    "        # Remove stopwords using spaCy\n",
    "        if INCLUDE_STOPWORDS:\n",
    "            df['cleaned_text'] = df[self.target_column].apply(lambda x: ' '.join(word for word in x.split() if word not in self.stop_words))\n",
    "        else:\n",
    "            df['cleaned_text'] = df[self.target_column].apply(lambda x: ' '.join(word for word in x.split()))\n",
    "        \n",
    "        # Tokenize cleaned texts\n",
    "        df['tokenized_text'] = df['cleaned_text'].apply(lambda text: self.tokenizer(text))\n",
    "        return df['tokenized_text']\n",
    "\n",
    "text_preprocessor = TextPreprocessor()\n",
    "df[TEXT] = text_preprocessor.clean_text(df.copy())\n",
    "df = df[df[TEXT].apply(lambda tokens: all(len(word) <= 100 for word in tokens))]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from torchtext.vocab import GloVe\n",
    "import matplotlib.pyplot as plt\n",
    "import nbformat\n",
    "\n",
    "def plot_updated_embeddings(df_embeddings, color_column='label', n=1000, name='gat'):\n",
    "    import plotly.graph_objs as go\n",
    "    import plotly.io as pio\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    pio.renderers.default = 'browser'\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Sample 'n' observations\n",
    "    df_sampled = df_embeddings.sample(n=n, random_state=42)\n",
    "\n",
    "    # Extract the embeddings and convert to a NumPy array\n",
    "    embedding_matrix = np.array(df_sampled['embedding'].tolist())\n",
    "\n",
    "    # Reduce the dimensionality to 3D using t-SNE\n",
    "    tsne = TSNE(n_components=3, perplexity=30, n_iter=300, random_state=42)\n",
    "    reduced_embeddings_tsne = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # Compute the mean and standard deviation of the reduced embeddings\n",
    "    mean = np.mean(reduced_embeddings_tsne, axis=0)\n",
    "    std = np.std(reduced_embeddings_tsne, axis=0)\n",
    "\n",
    "    # Compute the z-score for each point\n",
    "    z_scores = np.abs((reduced_embeddings_tsne - mean) / std)\n",
    "\n",
    "    # Set a threshold for outliers (e.g., z-score > 3)\n",
    "    threshold = 3  # adjust as needed\n",
    "    mask = (z_scores < threshold).all(axis=1)\n",
    "\n",
    "    # Filter the embeddings and other arrays\n",
    "    filtered_embeddings = reduced_embeddings_tsne[mask]\n",
    "    df_sampled_filtered = df_sampled[mask].reset_index(drop=True)\n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Prepare colors based on the specified column\n",
    "    unique_values = df_sampled_filtered[color_column].unique()\n",
    "    color_map = plt.cm.get_cmap('tab10', len(unique_values))\n",
    "\n",
    "    # Create a mapping from unique values to colors\n",
    "    color_dict = {value: color_map(i) for i, value in enumerate(unique_values)}\n",
    "\n",
    "    # Apply the mapping to the DataFrame column\n",
    "    colors = df_sampled_filtered[color_column].map(color_dict)\n",
    "\n",
    "    # Convert colors to a list\n",
    "    colors_list = colors.tolist()\n",
    "\n",
    "    # Create hover text for each point\n",
    "    hover_text = df_sampled_filtered.apply(\n",
    "        lambda row: f\"{color_column}: {row[color_column]}, Text: {' '.join(row[TEXT])}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Create a trace for the 3D scatter plot\n",
    "    trace = go.Scatter3d(\n",
    "        x=filtered_embeddings[:, 0],\n",
    "        y=filtered_embeddings[:, 1],\n",
    "        z=filtered_embeddings[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=3,\n",
    "            color=colors_list,  # Color based on the specified column\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=hover_text.tolist()  # Hover text\n",
    "    )\n",
    "\n",
    "    # Set up the layout\n",
    "    layout = go.Layout(\n",
    "        title=f't-SNE - {name} Embeddings colored by {color_column}',\n",
    "        scene=dict(\n",
    "            xaxis_title='Component 1',\n",
    "            yaxis_title='Component 2',\n",
    "            zaxis_title='Component 3'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "    # Show the plot\n",
    "    pio.show(fig)\n",
    "\n",
    "    # Save as HTML file\n",
    "    fig.write_html(f\"interactive_plot_{name}_{color_column}.html\")\n",
    "\n",
    "    # End Timer with note after\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for {n} points: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "if EMBEDDING_METHOD == \"Glove\":\n",
    "    glove = GloVe(name='6B', dim=EMBEDDING_DIM, cache=r'C:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\Sentence Embeddings\\.vector_cache')\n",
    "\n",
    "def get_average_embedding(tokens, glove, word2vec_model=None):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in glove.stoi:\n",
    "            embeddings.append(glove[token].numpy())  # Use GloVe if available\n",
    "        elif word2vec_model and token in word2vec_model.wv:\n",
    "            embeddings.append(word2vec_model.wv[token])  # Use Word2Vec as a fallback\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(glove.dim)  # Return a zero vector if no tokens are found\n",
    "    \n",
    "\n",
    "def get_bert_embedding(tokens):\n",
    "    # Join tokens back to text\n",
    "    text = ' '.join(tokens)\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=EMBEDDING_DIM)\n",
    "    outputs = bert_model(**inputs)\n",
    "    # Use the [CLS] token embedding\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "def get_bert_sentence_embedding(tokens):\n",
    "    # Join tokens back to text\n",
    "    text = ' '.join(tokens)\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=EMBEDDING_DIM)\n",
    "    outputs = bert_model(**inputs)\n",
    "    # Use the [CLS] token embedding\n",
    "    cls_embedding = outputs.last_hidden_state.squeeze().detach().numpy()\n",
    "    return cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23996, 18)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename text column to 'tokenized_text'\n",
    "\n",
    "if EMBEDDING_METHOD == \"BERT\":\n",
    "    df['embedding'] = df['text'].apply(get_bert_embedding)\n",
    "elif EMBEDDING_METHOD == \"Glove\":\n",
    "    df['embedding'] = df['text'].apply(lambda tokens: get_average_embedding(tokens, glove))\n",
    "elif EMBEDDING_METHOD == \"BERT_Sentence\":\n",
    "    df['embedding'] = df['text'].apply(get_bert_sentence_embedding)\n",
    "\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out Tweets with 'Unknown' Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    15320\n",
      "1     8676\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1162: FutureWarning:\n",
      "\n",
      "'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19872\\2410796214.py:51: MatplotlibDeprecationWarning:\n",
      "\n",
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 3000 points: 3.31 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert label column to integer type\n",
    "df['label'] = df['label'].astype(int)\n",
    "print(df['label'].value_counts())\n",
    "plot_updated_embeddings(df, color_column=PLOT_COLOUR, n=NUM_POINTS, name='before_gat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Tweet IDs and User IDs to Node Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from tweet IDs to indices\n",
    "tweet_id_to_idx = {tweet_id: idx for idx, tweet_id in enumerate(df['tweet_id'].unique())}\n",
    "df['tweet_idx'] = df['tweet_id'].map(tweet_id_to_idx)\n",
    "\n",
    "# Create a mapping from user IDs to indices\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(df['user_id'].unique())}\n",
    "df['user_idx'] = df['user_id'].map(user_id_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Heterogeneous Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Add tweet nodes with their embeddings\n",
    "data['tweet'].x = torch.tensor(np.stack(df['embedding'].values), dtype=torch.float)\n",
    "\n",
    "# Add user nodes (initialize with zero features)\n",
    "num_users = len(user_id_to_idx)\n",
    "data['user'].num_nodes = num_users\n",
    "data['user'].x = torch.zeros(num_users, data['tweet'].x.size(1))\n",
    "\n",
    "# Add 'writes' edges from users to tweets\n",
    "data['user', 'writes', 'tweet'].edge_index = torch.tensor([\n",
    "    df['user_idx'].values,\n",
    "    df['tweet_idx'].values\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Prepare the 'reply_to' edges\n",
    "reply_df = df[df['reply_to_tweet_id'].notnull()].copy()\n",
    "reply_df['reply_to_tweet_idx'] = reply_df['reply_to_tweet_id'].map(tweet_id_to_idx)\n",
    "reply_df = reply_df[reply_df['reply_to_tweet_idx'].notnull()].astype({'reply_to_tweet_idx': int})\n",
    "\n",
    "# Add 'reply_to' edges from tweets to tweets\n",
    "data['tweet', 'reply_to', 'tweet'].edge_index = torch.tensor([\n",
    "    reply_df['tweet_idx'].values,\n",
    "    reply_df['reply_to_tweet_idx'].values\n",
    "], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly for heterogeneous graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Display the figure\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m--> 118\u001b[0m \u001b[43mvisualize_original_graph_interactive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m, in \u001b[0;36mvisualize_original_graph_interactive\u001b[1;34m(data, edge_type, node_type)\u001b[0m\n\u001b[0;32m     34\u001b[0m node_sizes \u001b[38;5;241m=\u001b[39m [degree_dict[node] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes()]  \u001b[38;5;66;03m# Adjust scaling as needed\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Define layout using spring layout for better visualization\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Extract node positions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m node_x \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 4:4\u001b[0m, in \u001b[0;36margmap_spring_layout_1\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\networkx\\drawing\\layout.py:482\u001b[0m, in \u001b[0;36mspring_layout\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m    480\u001b[0m         nnodes, _ \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    481\u001b[0m         k \u001b[38;5;241m=\u001b[39m dom_size \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(nnodes)\n\u001b[1;32m--> 482\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[43m_sparse_fruchterman_reingold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    486\u001b[0m     A \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_numpy_array(G, weight\u001b[38;5;241m=\u001b[39mweight)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[0m, in \u001b[0;36margmap__sparse_fruchterman_reingold_5\u001b[1;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\networkx\\drawing\\layout.py:617\u001b[0m, in \u001b[0;36m_sparse_fruchterman_reingold\u001b[1;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[0;32m    615\u001b[0m delta \u001b[38;5;241m=\u001b[39m (pos[i] \u001b[38;5;241m-\u001b[39m pos)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# distance between points\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m distance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# enforce minimum distance of 0.01\u001b[39;00m\n\u001b[0;32m    619\u001b[0m distance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(distance \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, distance)\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:47\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "\n",
    "def visualize_original_graph_interactive(data, edge_type=('user', 'writes', 'tweet'), node_type='tweet'):\n",
    "    \"\"\"\n",
    "    Visualize the original graph interactively using Plotly with edge thickness and node sizes.\n",
    "\n",
    "    Args:\n",
    "        data (HeteroData): The heterogeneous graph data.\n",
    "        edge_type (tuple): The edge type to visualize.\n",
    "        node_type (str): The node type to set sizes.\n",
    "    \"\"\"\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    tweet_indices = data['tweet'].num_nodes\n",
    "    user_indices = data['user'].num_nodes\n",
    "    # Assign unique identifiers for nodes to distinguish user and tweet nodes\n",
    "    G.add_nodes_from([f\"user_{i}\" for i in range(user_indices)], type='user')\n",
    "    G.add_nodes_from([f\"tweet_{i}\" for i in range(tweet_indices)], type='tweet')\n",
    "\n",
    "    # Add edges\n",
    "    src, dst = data[edge_type].edge_index\n",
    "    edges = [(f\"user_{s.item()}\", f\"tweet_{d.item()}\") for s, d in zip(src, dst)]\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Compute edge widths (uniform if no weights are available)\n",
    "    edge_widths = [1 for _ in G.edges()]\n",
    "\n",
    "    # Compute node sizes based on degree (as a proxy for importance)\n",
    "    degree_dict = dict(G.degree())\n",
    "    node_sizes = [degree_dict[node] * 10 for node in G.nodes()]  # Adjust scaling as needed\n",
    "\n",
    "    # Define layout using spring layout for better visualization\n",
    "    pos = nx.spring_layout(G, k=0.15, iterations=20, seed=42)\n",
    "\n",
    "    # Extract node positions\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "\n",
    "    # Extract edge positions\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    # Create edge trace\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    # Create node trace\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale='YlGnBu',\n",
    "            reversescale=True,\n",
    "            color=[],  # To be filled with node degrees\n",
    "            size=node_sizes,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Degree',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "            line_width=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Assign colors based on node degree\n",
    "    node_adjacencies = [degree_dict[node] for node in G.nodes()]\n",
    "    node_trace.marker.color = node_adjacencies\n",
    "\n",
    "    # Create hover text\n",
    "    node_text = []\n",
    "    for node in G.nodes():\n",
    "        node_type_str, node_id = node.split('_')\n",
    "        text = f\"Type: {node_type_str}<br>ID: {node_id}<br>Degree: {degree_dict[node]}\"\n",
    "        node_text.append(text)\n",
    "    node_trace.text = node_text\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title='<br>Original Graph',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        annotations=[ dict(\n",
    "                            text=\"Interactive Network Graph\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\") ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                   )\n",
    "\n",
    "    # Optionally, save the plot as an HTML file\n",
    "    plot(fig, filename='original_graph_interactive.html')\n",
    "    # Display the figure\n",
    "    fig.show()\n",
    "\n",
    "visualize_original_graph_interactive(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Labels and Create Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels to tweet nodes\n",
    "data['tweet'].y = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "# Split indices for training, validation, and testing\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df.index, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_idx, test_size=0.25, random_state=42, stratify=df.loc[train_idx, 'label']\n",
    ")\n",
    "\n",
    "# Create boolean masks for the splits\n",
    "def create_mask(idx, size):\n",
    "    mask = torch.zeros(size, dtype=torch.bool)\n",
    "    mask[idx] = True\n",
    "    return mask\n",
    "\n",
    "data['tweet'].train_mask = create_mask(train_idx, len(df))\n",
    "data['tweet'].val_mask = create_mask(val_idx, len(df))\n",
    "data['tweet'].test_mask = create_mask(test_idx, len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Heterogeneous GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, HeteroConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class HeteroGATWithAttention(nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, heads=2, conv_type='GAT'):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGAT model with attention weight storage.\n",
    "\n",
    "        Args:\n",
    "            metadata: Metadata for the heterogeneous graph.\n",
    "            hidden_channels (int): Number of hidden units.\n",
    "            out_channels (int): Number of output units.\n",
    "            heads (int, optional): Number of attention heads. Defaults to 2.\n",
    "            conv_type (str, optional): Type of GAT convolution ('GAT' or 'GATv2'). Defaults to 'GAT'.\n",
    "        \"\"\"\n",
    "        super(HeteroGATWithAttention, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = heads\n",
    "        self.conv_type = conv_type\n",
    "\n",
    "        # Select the convolution class based on conv_type\n",
    "        if conv_type == 'GAT':\n",
    "            ConvLayer = GATConv\n",
    "        elif conv_type == 'GATv2':\n",
    "            ConvLayer = GATv2Conv\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported conv_type '{conv_type}'. Use 'GAT' or 'GATv2'.\")\n",
    "\n",
    "        # Define GAT layers for each relation type using the selected ConvLayer\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('tweet', 'reply_to', 'tweet'): ConvLayer((-1, -1), hidden_channels, heads=heads, add_self_loops=True),\n",
    "            ('user', 'writes', 'tweet'): ConvLayer((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('tweet', 'reply_to', 'tweet'): ConvLayer((-1, -1), hidden_channels, heads=heads, add_self_loops=True),\n",
    "            ('user', 'writes', 'tweet'): ConvLayer((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        # Linear layer for output (adjusted input dimension)\n",
    "        self.lin = nn.Linear(hidden_channels * heads, out_channels)\n",
    "\n",
    "        # To store attention weights\n",
    "        self.attention_weights = {}\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        \"\"\"\n",
    "        Forward pass of the HeteroGAT model. Stores attention weights.\n",
    "\n",
    "        Args:\n",
    "            x_dict (dict): Node feature dictionary.\n",
    "            edge_index_dict (dict): Edge index dictionary.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output for 'tweet' nodes.\n",
    "        \"\"\"\n",
    "        # First GAT layer\n",
    "        x_dict_updated, attn_weights_1 = self.conv1(x_dict, edge_index_dict)\n",
    "        # Store attention weights\n",
    "        self.attention_weights['layer1'] = attn_weights_1\n",
    "\n",
    "        # Merge updated features with original features\n",
    "        x_dict = {\n",
    "            key: F.elu(x_dict_updated[key][0].view(x_dict_updated[key][0].size(0), -1)) if key in x_dict_updated else x_dict[key]\n",
    "            for key in x_dict.keys()\n",
    "        }\n",
    "\n",
    "        # Second GAT layer\n",
    "        x_dict_updated, attn_weights_2 = self.conv2(x_dict, edge_index_dict)\n",
    "        # Store attention weights\n",
    "        self.attention_weights['layer2'] = attn_weights_2\n",
    "\n",
    "        x_dict = {\n",
    "            key: F.elu(x_dict_updated[key][0].view(x_dict_updated[key][0].size(0), -1)) if key in x_dict_updated else x_dict[key]\n",
    "            for key in x_dict.keys()\n",
    "        }\n",
    "\n",
    "        # Output layer for tweet nodes\n",
    "        out = self.lin(x_dict['tweet'])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model, Optimizer, and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\hetero_conv.py:76: UserWarning:\n",
      "\n",
      "There exist node types ({'user'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GAT\n",
    "#---------------------------------------------------------------\n",
    "# Get metadata from the data object\n",
    "metadata = data.metadata()\n",
    "\n",
    "# Instantiate the model\n",
    "model = HeteroGATWithAttention(metadata, hidden_channels=64, out_channels=2, conv_type='GAT')\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "#GATV2\n",
    "#---------------------------------------------------------------\n",
    "# Get metadata from the data object\n",
    "metadata_V2 = data.metadata()\n",
    "\n",
    "# Instantiate the model\n",
    "model_V2 = HeteroGATWithAttention(metadata, hidden_channels=64, out_channels=2, conv_type='GATv2')\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_V2 = torch.optim.Adam(model_V2.parameters(),  lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define the loss function\n",
    "criterion_V2 = nn.CrossEntropyLoss()\n",
    "#---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and Evaluation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "#--------------------------------------------------------------- \n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    out = out[data['tweet'].train_mask]\n",
    "    y = data['tweet'].y[data['tweet'].train_mask]\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(mask):\n",
    "    model.eval()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data['tweet'].y[mask]\n",
    "    accuracy = correct.sum().item() / mask.sum().item()\n",
    "    return accuracy\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# GATV2\n",
    "#---------------------------------------------------------------\n",
    "# Training function\n",
    "def train_V2():\n",
    "    model_V2.train()\n",
    "    optimizer_V2.zero_grad()\n",
    "    out = model_V2(data.x_dict, data.edge_index_dict)\n",
    "    out = out[data['tweet'].train_mask]\n",
    "    y = data['tweet'].y[data['tweet'].train_mask]\n",
    "    loss = criterion_V2(out, y)\n",
    "    loss.backward()\n",
    "    optimizer_V2.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate_V2(mask):\n",
    "    model_V2.eval()\n",
    "    out = model_V2(data.x_dict, data.edge_index_dict)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data['tweet'].y[mask]\n",
    "    accuracy = correct.sum().item() / mask.sum().item()\n",
    "    return accuracy\n",
    "#---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAT Model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining GAT Model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m evaluate(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_mask)\n\u001b[0;32m     22\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m evaluate(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mval_mask)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m out \u001b[38;5;241m=\u001b[39m out[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_mask]\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_mask]\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 59\u001b[0m, in \u001b[0;36mHeteroGATWithAttention.forward\u001b[1;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mForward pass of the HeteroGAT model. Stores attention weights.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output for 'tweet' nodes.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# First GAT layer\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m x_dict_updated, attn_weights_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x_dict, edge_index_dict)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Store attention weights\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m attn_weights_1\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training and Plotting\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Define the number of epochs\n",
    "epochs = EPOCHS\n",
    "\n",
    "# Initialize lists to store accuracy values for GAT\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# Initialize lists to store accuracy values for GATv2\n",
    "train_acc_list_V2 = []\n",
    "val_acc_list_V2 = []\n",
    "\n",
    "# Training Loop for GAT\n",
    "print(\"Training GAT Model:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc = evaluate(data['tweet'].train_mask)\n",
    "    val_acc = evaluate(data['tweet'].val_mask)\n",
    "    \n",
    "    # Append accuracies for plotting later\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    print(f'GAT Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Training Loop for GATv2\n",
    "print(\"\\nTraining GATv2 Model:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_V2()\n",
    "    train_acc = evaluate_V2(data['tweet'].train_mask)\n",
    "    val_acc = evaluate_V2(data['tweet'].val_mask)\n",
    "    \n",
    "    # Append accuracies for plotting later\n",
    "    train_acc_list_V2.append(train_acc)\n",
    "    val_acc_list_V2.append(val_acc)\n",
    "    \n",
    "    print(f'GATv2 Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Plotting the Accuracies Side by Side\n",
    "# ---------------------------------------------------------------\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6), sharey=True)\n",
    "\n",
    "# -----------------------\n",
    "# Plot GAT Accuracies\n",
    "# -----------------------\n",
    "axes[0].plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy', color='blue')\n",
    "axes[0].plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy', linestyle='--', color='orange')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('GAT: Train and Validation Accuracy over Epochs')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# -----------------------\n",
    "# Plot GATv2 Accuracies\n",
    "# -----------------------\n",
    "axes[1].plot(range(1, epochs + 1), train_acc_list_V2, label='Train Accuracy', color='green')\n",
    "axes[1].plot(range(1, epochs + 1), val_acc_list_V2, label='Validation Accuracy', linestyle='--', color='red')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "# Y-label is shared; no need to set for the second subplot\n",
    "axes[1].set_title('GATv2: Train and Validation Accuracy over Epochs')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1162: FutureWarning:\n",
      "\n",
      "'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_43664\\2410796214.py:51: MatplotlibDeprecationWarning:\n",
      "\n",
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 800 points: 0.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1162: FutureWarning:\n",
      "\n",
      "'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_43664\\2410796214.py:51: MatplotlibDeprecationWarning:\n",
      "\n",
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 800 points: 1.00 seconds\n"
     ]
    }
   ],
   "source": [
    "#GAT\n",
    "#---------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def get_tweet_embeddings(model, data):\n",
    "    model.eval()\n",
    "    x_dict = data.x_dict\n",
    "    edge_index_dict = data.edge_index_dict\n",
    "\n",
    "    # First GAT layer\n",
    "    x_dict_updated = model.conv1(x_dict, edge_index_dict)\n",
    "    x_dict = {\n",
    "        key: F.elu(x_dict_updated[key]) if key in x_dict_updated else x_dict[key]\n",
    "        for key in x_dict.keys()\n",
    "    }\n",
    "\n",
    "    # Second GAT layer\n",
    "    x_dict_updated = model.conv2(x_dict, edge_index_dict)\n",
    "    x_dict = {\n",
    "        key: F.elu(x_dict_updated[key]) if key in x_dict_updated else x_dict[key]\n",
    "        for key in x_dict.keys()\n",
    "    }\n",
    "\n",
    "    embeddings = x_dict['tweet']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Step 1: Extract embeddings\n",
    "embeddings = get_tweet_embeddings(model, data)\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "# Step 2: Prepare DataFrame with embeddings\n",
    "df_embeddings = df.copy()\n",
    "df_embeddings['embedding'] = list(embeddings_np)\n",
    "\n",
    "# Step 3: Plot the embeddings\n",
    "plot_updated_embeddings(df_embeddings, color_column=PLOT_COLOUR, n=NUM_POINTS,name='after_gat')\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "#GATV2\n",
    "#---------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def get_tweet_embeddings_V2(model, data):\n",
    "    model.eval()\n",
    "    x_dict = data.x_dict\n",
    "    edge_index_dict = data.edge_index_dict\n",
    "\n",
    "    # First GAT layer\n",
    "    x_dict_updated = model.conv1(x_dict, edge_index_dict)\n",
    "    x_dict = {\n",
    "        key: F.elu(x_dict_updated[key]) if key in x_dict_updated else x_dict[key]\n",
    "        for key in x_dict.keys()\n",
    "    }\n",
    "\n",
    "    # Second GAT layer\n",
    "    x_dict_updated = model.conv2(x_dict, edge_index_dict)\n",
    "    x_dict = {\n",
    "        key: F.elu(x_dict_updated[key]) if key in x_dict_updated else x_dict[key]\n",
    "        for key in x_dict.keys()\n",
    "    }\n",
    "\n",
    "    embeddings = x_dict['tweet']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Step 1: Extract embeddings\n",
    "embeddings_V2 = get_tweet_embeddings_V2(model_V2, data)\n",
    "embeddings_np_V2 = embeddings_V2.cpu().numpy()\n",
    "\n",
    "# Step 2: Prepare DataFrame with embeddings\n",
    "df_embeddings_V2 = df.copy()\n",
    "df_embeddings_V2['embedding'] = list(embeddings_np_V2)\n",
    "\n",
    "# Step 3: Plot the embeddings\n",
    "plot_updated_embeddings(df_embeddings_V2, color_column=PLOT_COLOUR, n=NUM_POINTS,name='after_gatv2')\n",
    "#---------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_original_graph_interactive(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\hetero_conv.py:76: UserWarning:\n",
      "\n",
      "There exist node types ({'user'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAT Model with Attention Extraction:\n",
      "GAT Attn Epoch 01, Loss: 0.6816, Train Acc: 0.5800, Val Acc: 0.5749\n",
      "GAT Attn Epoch 100, Loss: 0.3938, Train Acc: 0.8500, Val Acc: 0.7305\n",
      "GAT Attn Epoch 200, Loss: 0.1283, Train Acc: 0.9800, Val Acc: 0.7126\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[121], line 177\u001b[0m\n",
      "\u001b[0;32m    174\u001b[0m model_attn\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Extract attention weights\u001b[39;00m\n",
      "\u001b[1;32m--> 177\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attention_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Convert PyTorch Geometric data to NetworkX graph\u001b[39;00m\n",
      "\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# We'll manually construct the graph to include attention weights\u001b[39;00m\n",
      "\u001b[0;32m    181\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n",
      "\n",
      "Cell \u001b[1;32mIn[121], line 113\u001b[0m, in \u001b[0;36mHeteroGATWithAttention.get_attention_weights\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conv, GATConvWithAttention):\n",
      "\u001b[0;32m    112\u001b[0m         attention_weights\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "\u001b[1;32m--> 113\u001b[0m         attention_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m][edge_type] \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mcat(conv\u001b[38;5;241m.\u001b[39mdst_nodes, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Collect attention weights from conv2\u001b[39;00m\n",
      "\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge_type, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mconvs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, HeteroConv\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import remove_self_loops, to_networkx\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================\n",
    "# Your Existing Model Code\n",
    "# ============================\n",
    "\n",
    "# Define GATConvWithAttention\n",
    "class GATConvWithAttention(GATConv):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, **kwargs):\n",
    "        super(GATConvWithAttention, self).__init__(\n",
    "            in_channels, out_channels, heads=heads, concat=concat, **kwargs\n",
    "        )\n",
    "        self.attentions = []\n",
    "        self.dst_nodes = []\n",
    "\n",
    "    def message(self, **kwargs):\n",
    "        # Extract necessary components from kwargs\n",
    "        alpha = kwargs.get('alpha')  # Attention coefficients\n",
    "        edge_index_i = kwargs.get('edge_index_i')  # Destination node indices\n",
    "\n",
    "        if alpha is not None and edge_index_i is not None:\n",
    "            self.attentions.append(alpha)\n",
    "            self.dst_nodes.append(edge_index_i)\n",
    "\n",
    "        # Call the parent class's message method\n",
    "        return super(GATConvWithAttention, self).message(**kwargs)\n",
    "\n",
    "# Define HeteroGATWithAttention\n",
    "class HeteroGATWithAttention(nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, heads=2, conv_type='GAT'):\n",
    "        super(HeteroGATWithAttention, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = heads\n",
    "        self.conv_type = conv_type\n",
    "\n",
    "        # Select the convolution class based on conv_type\n",
    "        if conv_type == 'GAT':\n",
    "            ConvLayer = GATConvWithAttention  # Use the custom GATConvWithAttention\n",
    "        elif conv_type == 'GATv2':\n",
    "            ConvLayer = GATv2Conv  # Placeholder: Implement a similar custom class if needed\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported conv_type '{conv_type}'. Use 'GAT' or 'GATv2'.\")\n",
    "\n",
    "        # Define GAT layers for each relation type using the selected ConvLayer\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('tweet', 'reply_to', 'tweet'): ConvLayer(\n",
    "                (-1, -1), hidden_channels, heads=heads, add_self_loops=True\n",
    "            ),\n",
    "            ('user', 'writes', 'tweet'): ConvLayer(\n",
    "                (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n",
    "            ),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('tweet', 'reply_to', 'tweet'): ConvLayer(\n",
    "                (-1, -1), hidden_channels, heads=heads, add_self_loops=True\n",
    "            ),\n",
    "            ('user', 'writes', 'tweet'): ConvLayer(\n",
    "                (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n",
    "            ),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        # Linear layer for output (adjusted input dimension)\n",
    "        self.lin = nn.Linear(hidden_channels * heads, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Reset attention weights before forward pass\n",
    "        for conv in self.conv1.convs.values():\n",
    "            if isinstance(conv, GATConvWithAttention):\n",
    "                conv.attentions = []\n",
    "                conv.dst_nodes = []\n",
    "        for conv in self.conv2.convs.values():\n",
    "            if isinstance(conv, GATConvWithAttention):\n",
    "                conv.attentions = []\n",
    "                conv.dst_nodes = []\n",
    "\n",
    "        # First GAT layer\n",
    "        x_dict_updated = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {\n",
    "            key: F.elu(x_dict_updated[key]) if key in x_dict_updated else x_dict[key]\n",
    "            for key in x_dict.keys()\n",
    "        }\n",
    "\n",
    "        # Second GAT layer\n",
    "        x_dict_updated = self.conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {\n",
    "            key: F.elu(x_dict_updated[key]) if key in x_dict_updated else x_dict[key]\n",
    "            for key in x_dict.keys()\n",
    "        }\n",
    "\n",
    "        # Output layer for tweet nodes\n",
    "        out = self.lin(x_dict['tweet'])\n",
    "        return out\n",
    "\n",
    "    def get_attention_weights(self):\n",
    "        attention_weights = {}\n",
    "        # Collect attention weights from conv1\n",
    "        for edge_type, conv in self.conv1.convs.items():\n",
    "            if isinstance(conv, GATConvWithAttention):\n",
    "                attention_weights.setdefault('layer1', {})\n",
    "                attention_weights['layer1'][edge_type] = (torch.cat(conv.attentions, dim=0), torch.cat(conv.dst_nodes, dim=0))\n",
    "        # Collect attention weights from conv2\n",
    "        for edge_type, conv in self.conv2.convs.items():\n",
    "            if isinstance(conv, GATConvWithAttention):\n",
    "                attention_weights.setdefault('layer2', {})\n",
    "                attention_weights['layer2'][edge_type] = (torch.cat(conv.attentions, dim=0), torch.cat(conv.dst_nodes, dim=0))\n",
    "        return attention_weights\n",
    "\n",
    "# Initialize the HeteroGATWithAttention model\n",
    "# Note: Ensure that 'metadata', 'LR', 'WEIGHT_DECAY', 'EPOCHS', and 'data' are defined appropriately\n",
    "model_attn = HeteroGATWithAttention(metadata, hidden_channels=64, out_channels=2, conv_type='GAT')\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer_attn = torch.optim.Adam(model_attn.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion_attn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define training and evaluation functions\n",
    "def train_attn():\n",
    "    model_attn.train()\n",
    "    optimizer_attn.zero_grad()\n",
    "    out = model_attn(data.x_dict, data.edge_index_dict)\n",
    "    out = out[data['tweet'].train_mask]\n",
    "    y = data['tweet'].y[data['tweet'].train_mask]\n",
    "    loss = criterion_attn(out, y)\n",
    "    loss.backward()\n",
    "    optimizer_attn.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_attn(mask):\n",
    "    model_attn.eval()\n",
    "    out = model_attn(data.x_dict, data.edge_index_dict)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data['tweet'].y[mask]\n",
    "    accuracy = correct.sum().item() / mask.sum().item()\n",
    "    return accuracy\n",
    "\n",
    "# Training Loop\n",
    "epochs = EPOCHS\n",
    "train_acc_list_attn = []\n",
    "val_acc_list_attn = []\n",
    "\n",
    "print(\"Training GAT Model with Attention Extraction:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_attn()\n",
    "    train_acc = evaluate_attn(data['tweet'].train_mask)\n",
    "    val_acc = evaluate_attn(data['tweet'].val_mask)\n",
    "    \n",
    "    # Append accuracies for plotting later\n",
    "    train_acc_list_attn.append(train_acc)\n",
    "    val_acc_list_attn.append(val_acc)\n",
    "    \n",
    "    # Print progress every 100 epochs and the first epoch\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f'GAT Attn Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# ============================\n",
    "# Visualization Code\n",
    "# ============================\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model_attn.eval()\n",
    "\n",
    "# Extract attention weights\n",
    "attention_weights = model_attn.get_attention_weights()\n",
    "\n",
    "# Convert PyTorch Geometric data to NetworkX graph\n",
    "# We'll manually construct the graph to include attention weights\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all nodes from the 'tweet' and 'user' node types\n",
    "# Assuming node indices are unique across types or appropriately mapped\n",
    "# Adjust this part if node indices overlap or need specific handling\n",
    "for node_type in data.x_dict:\n",
    "    num_nodes = data.x_dict[node_type].size(0)\n",
    "    G.add_nodes_from([f\"{node_type}_{i}\" for i in range(num_nodes)], node_type=node_type)\n",
    "\n",
    "# Initialize a dictionary to hold edge weights\n",
    "edge_weights = defaultdict(list)\n",
    "\n",
    "# Iterate through each layer and edge type to collect attention weights\n",
    "for layer, edge_types in attention_weights.items():\n",
    "    for edge_type, (alphas, dst_nodes_attn) in edge_types.items():\n",
    "        src_node_type, relation, dst_node_type = edge_type\n",
    "        edge_index = data.edge_index_dict[edge_type].cpu().numpy()  # Shape: [2, num_edges]\n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "        alphas = alphas.cpu().numpy()\n",
    "        dst_nodes_attn = dst_nodes_attn.cpu().numpy()\n",
    "\n",
    "        # Assuming the order of alphas corresponds to the order of edges in edge_index\n",
    "        for src, dst, alpha in zip(src_nodes, dst_nodes, alphas):\n",
    "            # Create unique node identifiers\n",
    "            src_node = f\"{src_node_type}_{src}\"\n",
    "            dst_node = f\"{dst_node_type}_{dst}\"\n",
    "            # Use a tuple of node identifiers as the edge key\n",
    "            edge_key = (src_node, dst_node)\n",
    "            edge_weights[edge_key].append(alpha)\n",
    "\n",
    "# Assign average attention weights to edges\n",
    "for edge, alphas in edge_weights.items():\n",
    "    avg_alpha = np.mean(alphas)\n",
    "    G.add_edge(edge[0], edge[1], weight=avg_alpha)\n",
    "\n",
    "# Normalize edge weights for visualization (thickness)\n",
    "weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "if weights:\n",
    "    max_weight = max(weights)\n",
    "    min_weight = min(weights)\n",
    "else:\n",
    "    max_weight = 1\n",
    "    min_weight = 0\n",
    "\n",
    "# Define edge thickness range\n",
    "min_thickness = 1\n",
    "max_thickness = 10\n",
    "\n",
    "# Normalize weights to thickness\n",
    "normalized_weights = [\n",
    "    min_thickness + (w - min_weight) / (max_weight - min_weight) * (max_thickness - min_thickness)\n",
    "    if max_weight != min_weight else min_thickness\n",
    "    for w in weights\n",
    "]\n",
    "\n",
    "# Assign normalized thickness to edges\n",
    "for (edge, thickness) in zip(G.edges(), normalized_weights):\n",
    "    G.edges[edge]['thickness'] = thickness\n",
    "\n",
    "# Determine node sizes based on the sum of incoming attention weights\n",
    "node_sizes = {}\n",
    "for node in G.nodes():\n",
    "    incoming_weights = [G[u][node]['weight'] for u in G.predecessors(node)] if G.is_directed() else [G[u][node]['weight'] for u in G.neighbors(node)]\n",
    "    total_weight = sum(incoming_weights) if incoming_weights else 1\n",
    "    node_sizes[node] = total_weight\n",
    "\n",
    "# Normalize node sizes for visualization\n",
    "sizes = list(node_sizes.values())\n",
    "if sizes:\n",
    "    max_size = max(sizes)\n",
    "    min_size = min(sizes)\n",
    "else:\n",
    "    max_size = 1\n",
    "    min_size = 0\n",
    "\n",
    "# Define node size range\n",
    "min_node_size = 10\n",
    "max_node_size = 50\n",
    "\n",
    "normalized_node_sizes = [\n",
    "    min_node_size + (s - min_size) / (max_size - min_size) * (max_node_size - min_node_size)\n",
    "    if max_size != min_size else min_node_size\n",
    "    for s in sizes\n",
    "]\n",
    "\n",
    "# Assign normalized sizes to nodes\n",
    "for node, size in zip(G.nodes(), normalized_node_sizes):\n",
    "    G.nodes[node]['size'] = size\n",
    "\n",
    "# Create positions for all nodes using a layout algorithm\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20, seed=42)  # Seed for reproducibility\n",
    "\n",
    "# Prepare edge traces grouped by thickness to minimize the number of Plotly traces\n",
    "thickness_to_edges = defaultdict(list)\n",
    "for edge in G.edges(data=True):\n",
    "    thickness = edge[2]['thickness']\n",
    "    thickness_to_edges[thickness].append(edge)\n",
    "\n",
    "# Create Plotly edge traces\n",
    "edge_traces = []\n",
    "for thickness, edges in thickness_to_edges.items():\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in edges:\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "    trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        line=dict(width=thickness, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "    edge_traces.append(trace)\n",
    "\n",
    "# Extract node information\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_size = []\n",
    "node_color = []\n",
    "node_text = []\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    x, y = pos[node[0]]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_size.append(node[1]['size'])\n",
    "    node_color.append(node[1]['size'])\n",
    "    node_text.append(f'Node: {node[0]}<br>Size: {node[1][\"size\"]:.2f}')\n",
    "\n",
    "# Create node trace\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        color=node_color,\n",
    "        size=node_size,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Node Size',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2\n",
    "    ),\n",
    "    text=node_text\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add all edge traces\n",
    "for trace in edge_traces:\n",
    "    fig.add_trace(trace)\n",
    "\n",
    "# Add node trace\n",
    "fig.add_trace(node_trace)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Network Graph with Attention-based Edge Thickness and Node Sizes',\n",
    "    titlefont_size=16,\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    margin=dict(b=20, l=5, r=5, t=40),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"\",\n",
    "            showarrow=False,\n",
    "            xref=\"paper\", yref=\"paper\"\n",
    "        )\n",
    "    ],\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n",
    "\n",
    "# Optional: Save the plot as an HTML file\n",
    "# plot(fig, filename='network_graph.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for base GAT: 0.7305\n",
      "Test Accuracy for GATv2: 0.6826\n"
     ]
    }
   ],
   "source": [
    "#GAT\n",
    "#---------------------------------------------------------------\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(data['tweet'].test_mask)\n",
    "print(f'Test Accuracy for base GAT: {test_acc:.4f}')\n",
    "#------------------------------------------------\n",
    "\n",
    "#GATV2\n",
    "#---------------------------------------------------------------\n",
    "# Evaluate the model on the test set\n",
    "test_acc_V2 = evaluate_V2(data['tweet'].test_mask)\n",
    "print(f'Test Accuracy for GATv2: {test_acc_V2:.4f}')\n",
    "#---------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
